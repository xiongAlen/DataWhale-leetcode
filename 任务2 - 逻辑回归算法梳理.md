### 任务2 - 逻辑回归算法梳理

##### 1、逻辑回归与线性回归的联系与区别

>  联系：

线性回归+sigmoid函数=逻辑回归

> 区别：

（1）功能不同：

线性回归是做回归的，逻辑回归是做分类的。

（2）参数求解方法不同：

线性回归是用最小二乘法求解参数，逻辑回归是用梯度上升法求解参数。



#####  2、 逻辑回归的原理 

> 逻辑回归（Logistic Regression**）**是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。



##### 3、逻辑回归损失函数推导及优化

参考链接：<https://blog.csdn.net/fu_9701/article/details/83097176>



#####  4、 正则化与模型评估指标 

>  正则化（Regularization）

机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作-norm和-norm，中文称作L1正则化和L2正则化，或者L1范数和L2范数。

L1正则化和L2正则化可以看做是损失函数的惩罚项。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。

参考链接：<https://blog.csdn.net/panghaomingme/article/details/54022709>

##### 5、逻辑回归的优缺点 

优点：*1*）适合需要得到一个分类概率的场景。*2*）计算代价不高，容易理解实现。*LR*在时间和内存需求上相当高效。它可以应用于分布式数据，并且还有在线算法实现，用较少的资源处理大型数据。*3*）*LR*对于数据中小噪声的鲁棒性很好，并且不会受到轻微的多重共线性的特别影响。（严重的多重共线性则可以使用逻辑回归结合*L2*正则化来解决，但是若要得到一个简约模型，*L2*正则化并不是最好的选择，因为它建立的模型涵盖了全部的特征。）

   缺点：1）容易欠拟合，分类精度不高。*2*）数据特征有缺失或者特征空间很大时表现效果并不好。



##### 6、样本不均衡问题解决办法 

1. 产生新数据型：过采样小样本(SMOTE)，欠采样大样本。
2. 对原数据的权值进行改变
3. 通过组合集成方法解决

4. 通过特征选择

参考链接：<https://www.jianshu.com/p/76dce1fca85b>



7. ##### sklearn参数

   ```
   LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’liblinear’, max_iter=100, multi_class=’ovr’, verbose=0, warm_start=False, n_jobs=1)
   
   参数介绍 
   penalty惩罚项 
   str, ‘l1’ or ‘l2’, 
   默认: ‘l2’ 
   注：在调参时如果我们主要的目的只是为了解决过拟合，一般penalty选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。
   
   solver优化方法 
   （1）liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。 
   （2）lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 
   （3）newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 
   （4）sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候，SAG是一种线性收敛算法，这个速度远比SGD快。 
   注：从上面的描述可以看出，newton-cg, lbfgs和sag这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的L1正则化，只能用于L2正则化。而liblinear通吃L1正则化和L2正则化。
   
   multi_class:（待完善） 
   ‘ovr’:uses the one-vs-rest (OvR) scheme，无论多少元逻辑回归都看作二元的 
   ‘multinomial’:uses the cross- entropy loss
   
   正则化参数C 
   smaller values specify stronger regularization
   ```

   