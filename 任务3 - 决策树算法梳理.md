

### 任务3 - 决策树算法梳理



#### 1、信息论基础（熵 联合熵 条件熵 信息增益 基尼不纯度） 2.决策树的不同分类算法（ID3算法、C4.5、CART分类树）的原理及应用场景

> 熵

熵是对随机变量不确定性的度量，是信息的期望值，熵只依赖于随机变量的分布，和其取值没有关系
熵是用来度量不确定的，所以熵越大，X=xi的不确定性越大。



> 联合熵

一维随机变量分布推广到多维随机变量分布。



> 条件熵

在一个条件下，随机变量的不确定性。机器学习中可以理解为给定某个特征后的熵。



> 信息增益

以某特征划分数据集前后的熵的差值。即待分类集合的熵和选定某个特征的条件熵之差。



> 基尼不纯度

Gini系数是一种与信息熵类似的做特征选择的方式，可以用来数据的不纯度。在CART(Classification and Regression Tree)算法中利用基尼指数构造二叉决策树（选择基尼系数最小的特征及其对应的特征值）。

基尼指数（基尼不纯度）：表示在样本集合中一个随机选中的样本被分错的概率。



> 决策树的不同分类算法

**ID3**：采用信息增益划分数据。计算使用所有特征划分数据集，得到多个特征划分数据集的信息增益，从这些信息增益中选择最大的，因而当前结点的划分特征便是使信息增益最大的划分所使用的特征。

**C4.5**：采用信息增益比划分数据，弥补ID3的不足

**CART**：采用基尼系数划分数据，可针对离散和连续型，可以做分类和回归

参考链接：<https://www.wandouip.com/t5i39653/>



#### 2、回归树原理

决策树最直观的理解其实就是，输入特征空间(RnRn)，然后对特征空间做划分，每一个划分属于同一类或者对于一个输出的预测值。那么这个算法需要解决的问题是1. 如何决策边界(划分点)？2. 尽可能少的比较次数(决策树的形状)

参考链接：<https://www.cnblogs.com/xiemaycherry/p/10540902.html>



#### 3、决策树防止过拟合手段

合理、有效地抽样，用相对能够反映业务逻辑的训练集去产生决策树；

剪枝：提前停止树的增长或者对已经生成的树按照一定的规则进行后剪枝。

参考链接：<https://blog.csdn.net/sinat_32043495/article/details/78729610>



#### 4、模型评估

评估指标有分类准确度、召回率、虚警率和精确度等。而这些指标都是基于混淆矩阵 (confusion matrix) 进行计算的。混淆矩阵是用来评价监督式学习模型的精确性，矩阵的每一列代表一个类的实例预测，而每一行表示一个实际的类的实例。



#### 5、sklearn参数详解，Python绘制决策树

```javascript
sklearn.tree.DecisionTreeClassifier
        (criterion='gini', splitter='best', max_depth=None, min_samples_split=2, 
        min_samples_leaf=1,min_weight_fraction_leaf=0.0, max_features=None, 
        random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, 
        min_impurity_split=None, class_weight=None, presort=False)

criterion:特征选择的标准，有信息增益和基尼系数两种，使用信息增益的是ID3和C4.5算法（使用信息增益比），使用基尼系数的CART算法，默认是gini系数。

splitter:特征切分点选择标准，决策树是递归地选择最优切分点，spliter是用来指明在哪个集合上来递归，有“best”和“random”两种参数可以选择，best表示在所有特征上递归，适用于数据集较小的时候，random表示随机选择一部分特征进行递归，适用于数据集较大的时候。

max_depth:决策树最大深度，决策树模型先对所有数据集进行切分，再在子数据集上继续循环这个切分过程，max_depth可以理解成用来限制这个循环次数。

min_samples_split:子数据集再切分需要的最小样本量，默认是2，如果子数据样本量小于2时，则不再进行下一步切分。如果数据量较小，使用默认值就可，如果数据量较大，为降低计算量，应该把这个值增大，即限制子数据集的切分次数。

min_samples_leaf:叶节点（子数据集）最小样本数，如果子数据集中的样本数小于这个值，那么该叶节点和其兄弟节点都会被剪枝（去掉），该值默认为1。

min_weight_fraction_leaf:在叶节点处的所有输入样本权重总和的最小加权分数，如果不输入则表示所有的叶节点的权重是一致的。

max_features:特征切分时考虑的最大特征数量，默认是对所有特征进行切分，也可以传入int类型的值，表示具体的特征个数；也可以是浮点数，则表示特征个数的百分比；还可以是sqrt,表示总特征数的平方根；也可以是log2，表示总特征数的log个特征。

random_state:随机种子的设置，与LR中参数一致。

max_leaf_nodes:最大叶节点个数，即数据集切分成子数据集的最大个数。

min_impurity_decrease:切分点不纯度最小减少程度，如果某个结点的不纯度减少小于这个值，那么该切分点就会被移除。

min_impurity_split:切分点最小不纯度，用来限制数据集的继续切分（决策树的生成），如果某个节点的不纯度（可以理解为分类错误率）小于这个阈值，那么该点的数据将不再进行切分。

class_weight:权重设置，主要是用于处理不平衡样本，与LR模型中的参数一致，可以自定义类别权重，也可以直接使用balanced参数值进行不平衡样本处理。

presort:是否进行预排序，默认是False，所谓预排序就是提前对特征进行排序，我们知道，决策树分割数据集的依据是，优先按照信息增益/基尼系数大的特征来进行分割的，涉及的大小就需要比较，如果不进行预排序，则会在每次分割的时候需要重新把所有特征进行计算比较一次，如果进行了预排序以后，则每次分割的时候，只需要拿排名靠前的特征就可以了。
```

参考链接：

<https://cloud.tencent.com/developer/article/1146079>

<http://www.4u4v.net/hui-gui-shu-di-yuan-li-ji-qi-python-shi-xian.html>