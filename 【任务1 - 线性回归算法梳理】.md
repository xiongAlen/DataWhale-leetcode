### 【任务1 - 线性回归算法梳理】

#### 1、机器学习的常见一些概念

>  有监督、无监督

```
监督学习，通俗来讲就是分类，就是把训练样本，在某种评价下得到最佳的模型，然后再利用这个模型将输入映射为相应的输出，对输出进行简单的判断从而实现分类的目的。
无监督学习，输入数据没有被标记，也没有确定的结果。样本数据类别未知，需要根据样本间的相似性对样本集进行分类（聚类，clustering）试图使类内差距最小化，类间差距最大化。
```

> 泛化能力

```
泛化能力就是模型对未知数据的预测能力。在实际当中，我们通常通过测试误差来评价学习方法的泛化能力。
```

> 过拟合欠拟合(方差和偏差以及各自解决办法)

```
过拟合：建的机器学习模型在训练样本中表现得过于优越，导致在验证数据集以及测试数据集中表现不佳。
解决办法：1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。3）采用正则化方法。正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对于的范数。但是在机器学习中一般使用L2正则。
欠拟合：提取的特征比较少，导致训练出来的模型不能很好地匹配。
解决办法：1）添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。2）添加多项式特征。3）减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。
```

> 交叉验证

```
在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对分类器进行训练,再利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。
```

#### 2、线性回归的原理

> 一种通过属性的线性组合来进行预测的线性模型，其目的是找到一条直线或者一个平面或者更高维的超平面，使得预测值与真实值之间的误差最小化。

#### 3、线性回归损失函数、代价函数、目标函数

> 损失函数：计算的是一个样本的误差
>
> 代价函数：是整个训练集上所有样本误差的平均
>
> 目标函数：代价函数 + 正则化项

参考链接：<http://www.ai-start.com/ml2014/html/week1.html>

#### 4、优化方法(梯度下降法、牛顿法、拟牛顿法等)

> 梯度下降法

```
批量梯度下降法（Batch Gradient Descent）: 梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新

随机梯度下降法（Stochastic Gradient Descent）: 和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。

 小批量梯度下降法（Mini-batch Gradient Descent）: 批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，1<x<m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。
 
 参考链接：https://www.cnblogs.com/yhll/p/9238383.html
```

> 牛顿法、拟牛顿法

```
牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。

参考链接：https://www.cnblogs.com/shixiangwan/p/7532830.html
```

#### 5、线性回归的评估指标

> 均方误差（MSE）

MSE （Mean Squared Error）叫做均方误差。用 真实值-预测值 然后平方之后求和平均。

```
y_preditc=reg.predict(x_test) #reg是训练好的模型
mse_test=np.sum((y_preditc-y_test)**2)/len(y_test) #跟数学公式一样的
```



> 均方根误差（RMSE）

RMSE（Root Mean Squard Error）均方根误差。MSE开个根号。

```
rmse_test=mse_test ** 0.5
```



> MAE

MAE(平均绝对误差)

```
mae_test=np.sum(np.absolute(y_preditc-y_test))/len(y_test)
```



> R Squared

组内变异（SSE）+组间变异（SSA）=总变异（SST），可以推出公式R squared=1-SSE/SST

```
1- mean_squared_error(y_test,y_preditc)/ np.var(y_test)
```

参考链接：<https://www.jianshu.com/p/9ee85fdad150>



#### 6、sklearn参数详解

```
from sklearn.linear_model import LinearRegression
LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)
'''
参数含义：
1.fit_intercept:布尔值，指定是否需要计算线性回归中的截距，即b值。如果为False,
那么不计算b值。
2.normalize:布尔值。如果为False，那么训练样本会进行归一化处理。
3.copy_X：布尔值。如果为True，会复制一份训练数据。
4.n_jobs:一个整数。任务并行时指定的CPU数量。如果取值为-1则使用所有可用的CPU。
返回值：
5.coef_:权重向量
6.intercept_:截距b值
 
方法：
1.fit(X,y)：训练模型。
2.predict(X)：用训练好的模型进行预测，并返回预测值。
3.score(X,y)：返回预测性能的得分。计算公式为：score=(1 - u/v)
其中u=((y_true - y_pred) ** 2).sum()，v=((y_true - y_true.mean()) ** 2).sum()
score最大值是1，但有可能是负值(预测效果太差)。score越大，预测性能越好。
'''
参考链接：https://blog.csdn.net/voidfaceless/article/details/61182436
```

